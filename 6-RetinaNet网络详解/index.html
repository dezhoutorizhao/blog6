<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>RetinaNet网络详解 | h3110w0r1d's Blog</title><meta name="author" content="h3110w0r1d"><meta name="copyright" content="h3110w0r1d"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="摘要 Retinanet 是作者 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection.  作者深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数">
<meta property="og:type" content="article">
<meta property="og:title" content="RetinaNet网络详解">
<meta property="og:url" content="http://dezhoutorizhao.github.io/6-RetinaNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/index.html">
<meta property="og:site_name" content="h3110w0r1d&#39;s Blog">
<meta property="og:description" content="摘要 Retinanet 是作者 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection.  作者深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://strongwillpro.oss-cn-beijing.aliyuncs.com/img/QQ图片20240111033823.jpg">
<meta property="article:published_time" content="2025-02-03T20:02:51.459Z">
<meta property="article:modified_time" content="2025-02-03T20:05:15.631Z">
<meta property="article:author" content="h3110w0r1d">
<meta property="article:tag" content="勇敢的面对阳光，阴影自然都在身后">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://strongwillpro.oss-cn-beijing.aliyuncs.com/img/QQ图片20240111033823.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://dezhoutorizhao.github.io/6-RetinaNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.0.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'RetinaNet网络详解',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-02-04 04:05:15'
}</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (true) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image: url(https://strongwillpro.oss-cn-beijing.aliyuncs.com/img/background.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://strongwillpro.oss-cn-beijing.aliyuncs.com/img/QQ图片20240111033823.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">113</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">64</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background: transparent;"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">h3110w0r1d's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">RetinaNet网络详解</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">RetinaNet网络详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-03T20:02:51.459Z" title="发表于 2025-02-04 04:02:51">2025-02-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-02-03T20:05:15.631Z" title="更新于 2025-02-04 04:05:15">2025-02-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>31分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote>
<p>Retinanet 是作者 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection.</p>
</blockquote>
<p>作者深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。</p>
<h2 id="1，引言"><a href="#1，引言" class="headerlink" title="1，引言"></a>1，引言</h2><p>作者认为一阶段检测器的精度不能和两阶段检测相比的原因主要在于，训练过程中的类别不平衡，由此提出了一种新的损失函数-<code>Focal Loss</code>。</p>
<p><code>R-CNN(Fast RCNN)</code> 类似的检测器之所以能解决类别不平衡问题，是因为<strong>两阶段级联结构和启发式采样</strong>。提取 <code>proposal</code> 阶段（例如，选择性搜索、EdgeBoxes、DeepMask、<code>RPN</code>）很快的将候选对象位置的数量缩小到一个小数目（例如，1-2k），过滤掉大多数背景样本（其实就是筛选 <code>anchor</code> 数量）。在第二个分类阶段，执行启发式采样（<code>sampling heuristics</code>），例如固定的前景背景比（<code>1:3</code>），或在线难样本挖掘（<code>OHEM</code>），以保持前景和背景之间的平衡。</p>
<p>相比之下，单级检测器必须处理在图像中定期采样的一组更大的候选对象位置。实际上，这通常相当于枚举 <code>∼100k</code> 个位置，这些位置密集地覆盖空间位置、尺度和纵横。虽然也可以应用类似的启发式采样方法，但效率低下，因为训练过程仍然由易于分类的背景样本主导。</p>
<h2 id="2，相关工作"><a href="#2，相关工作" class="headerlink" title="2，相关工作"></a>2，相关工作</h2><p><strong>Two-stage Detectors</strong>: 与之前使用两阶段的分类器生成 <code>proposal</code> 不同，<code>Faster RCNN</code> 模型的 <code>RPN</code> 使用单个卷积就可以生成 <code>proposal</code>。</p>
<p><strong>One-stage Detectors</strong>：最近的一些研究表明，只需要降低输入图像分辨率和 <code>proposal</code> 数量，两阶段检测器速度就可以变得更快。但是，对于一阶段检测器，即使提高模型计算量，其最后的精度也落后于两阶段方法[17]。同时，作者强调，<code>Reinanet</code> 达到很好的结果的原因不在于网络结构的创新，而在于损失函数的创新。</p>
<blockquote>
<p>论文 [17] Speed/accuracy trade-offs for modern convolutional object detectors（注重实验）. 但是，从这几年看，一阶段检测器也可以达到很高的精度，甚至超过两阶段检测器，这几年的一阶段检测和两阶段检测器有相互融合的趋势了。</p>
</blockquote>
<p><strong><code>Class Imbalance:</code></strong> 早期的目标检测器 <code>SSD</code> 等在训练过程中会面临严重的类别不平衡（<code>class imbalance</code>）的问题，即正样本太少，负样本太多，这会导致两个问题：</p>
<ul>
<li>训练效率低下：大多数候选区域都是容易分类的负样本，并没有提供多少有用的学习信号。</li>
<li>模型退化：易分类的负样本太多会压倒训练，导致模型退化。</li>
</ul>
<p>通常的解决方案是执行某种形式的<strong>难负样本挖掘</strong>，如在训练时进行难负样本采样或更复杂的采样/重新称重方案。相比之下，<code>Focla Loss</code> 自然地处理了单级检测器所面临的类别不平衡，并且<strong>允许在所有示例上有效地训练</strong>，而不需要采样，也不需要容易的负样本来压倒损失和计算的梯度。</p>
<p><strong>Robust Estimation</strong>: 人们对设计稳健的损失函数（例如 <code>Huber loss</code>）很感兴趣，该函数通过降低具有大错误的示例（硬示例）的损失来减少对总损失的贡献。相反， <code>Focal Loss</code> 对容易样本(<code>inliers</code>)减少权重来解决（<code>address</code>）类别不平衡问题（<code>class imbalance</code>），这意味着即使容易样本数量大，但是其对总的损失函数贡献也很小。换句话说，<code>Focal Loss</code> 与鲁棒损失相反，它<strong>侧重于训练稀疏的难样本</strong>。</p>
<h2 id="3，网络架构"><a href="#3，网络架构" class="headerlink" title="3，网络架构"></a>3，网络架构</h2><p><code>retinanet</code> 的网络架构图如下所示。</p>
<p><img src="../../data/images/retinanet/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="网络架构图"></p>
<h2 id="3-1，Backbone"><a href="#3-1，Backbone" class="headerlink" title="3.1，Backbone"></a>3.1，Backbone</h2><p><code>Retinanet</code> 的 <code>Backbone</code> 为 <code>ResNet</code> 网络，<code>ResNet</code> 一般从 <code>18</code> 层到 <code>152</code> 层（甚至更多）不等，主要区别在于采用的残差单元/模块不同或者堆叠残差单元/模块的数量和比例不同，论文主要使用 <code>ResNet50</code>。</p>
<p>两种残差块结构如下图所示，<code>ResNet50</code> 及更深的 <code>ResNet</code> 网络使用的是 <code>bottleneck</code> 残差块。</p>
<p><img src="../../data/images/retinanet/%E4%B8%A4%E7%A7%8D%E6%AE%8B%E5%B7%AE%E5%9D%97%E7%BB%93%E6%9E%84.png" alt="两种残差块结构"></p>
<h3 id="3-2，Neck"><a href="#3-2，Neck" class="headerlink" title="3.2，Neck"></a>3.2，Neck</h3><p><code>Neck</code> 模块即为 <code>FPN</code> 网络结构。FPN 模块接收 c3, c4, c5 三个特征图，输出 P2-P7 五个特征图，通道数都是 256, stride 为 (8,16,32,64,128)，<strong>其中大 stride (特征图小)用于检测大物体，小 stride (特征图大)用于检测小物体</strong>。P6 和 P7 目的是提供一个<strong>大感受野强语义</strong>的特征图，有利于大物体和超大物体检测。注意：在 RetinaNet 的 FPN 模块中只包括卷积，不包括 BN 和 ReLU。</p>
<h3 id="3-3，Head"><a href="#3-3，Head" class="headerlink" title="3.3，Head"></a>3.3，Head</h3><p><code>Head</code> 即预测头网络。</p>
<p><code>YOLOv3</code> 的 <code>neck</code> 输出 <code>3</code> 个分支，即输出 <code>3</code> 个特征图， <code>head</code> 模块只有一个分支，由卷积层组成，该卷积层完成目标分类和位置回归的功能。总的来说，<code>YOLOv3</code> 网络的 <code>3</code> 个特征图有 <code>3</code> 个预测分支，分别预测 <code>3</code> 个框，也就是分别预测大、中、小目标。</p>
<p><code>Retinanet</code> 的 <code>neck</code> 输出 <code>5</code> 个分支，即输出 <code>5</code> 个特征图。<code>head</code> 模块包括分类和位置检测两个分支，每个分支都包括 <code>4</code> 个卷积层，但是 <code>head</code> 模块的这两个分支之间参数不共享，分类 <code>Head</code> 输出通道是 A*K，A 是类别数；检测 <code>head</code> 输出通道是 4*K, K 是 anchor 个数, 虽然每个 Head 的分类和回归分支权重不共享，但是 <code>5</code> 个输出特征图的 Head 模块权重是共享的。</p>
<h2 id="4，Focal-Loss"><a href="#4，Focal-Loss" class="headerlink" title="4，Focal Loss"></a>4，Focal Loss</h2><p><code>Focal Loss</code> 是在二分类问题的交叉熵（<code>CE</code>）损失函数的基础上引入的，所以需要先学习下交叉熵损失的定义。</p>
<h3 id="4-1，Cross-Entropy"><a href="#4-1，Cross-Entropy" class="headerlink" title="4.1，Cross Entropy"></a>4.1，Cross Entropy</h3><blockquote>
<p>可额外阅读文章 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/339684056">理解交叉熵损失函数</a>。</p>
</blockquote>
<p>在深度学习中我们常使用交叉熵来作为分类任务中训练数据分布和模型预测结果分布间的代价函数。对于同一个离散型随机变量 $\textrm{x}$ 有两个单独的概率分布 $P(x)$ 和 $Q(x)$，其交叉熵定义为：</p>
<blockquote>
<p>P 表示真实分布， Q 表示预测分布。</p>
</blockquote>
<p>$$H(P,Q) = \mathbb{E}<em>{\textrm{x}\sim P} log Q(x)= -\sum</em>{i}P(x_i)logQ(x_i) \tag{1} $$</p>
<p>但在实际计算中，我们通常不这样写，因为不直观。在深度学习中，以二分类问题为例，其交叉熵损失（<code>CE</code>）函数如下：</p>
<p>$$Loss = L(y, p) = -ylog(p)-(1-y)log(1-p) \tag{2}$$</p>
<p>其中 $p$ 表示当预测样本等于 $1$ 的概率，则 $1-p$ 表示样本等于 $0$ 的预测概率。因为是二分类，所以样本标签 $y$ 取值为 ${1,0}$，上式可缩写至如下：</p>
<p>$$CE = \left{\begin{matrix}<br>-log(p), &amp; if \quad y=1 \<br>-log(1-p), &amp;  if\quad y=0  \tag{3}<br>\end{matrix}\right.$$</p>
<p>为了方便，用 $p_t$ 代表 $p$，$p_t$ 定义如下：</p>
<p>$$p_t = \left{\begin{matrix}<br>p, &amp; if \quad y=1 \<br>1-p, &amp;  if\quad y=0<br>\end{matrix}\right.$$</p>
<p>则$(3)$式可写成：</p>
<p>$$CE(p, y) = CE(p_t) = -log(p_t) \tag{4}$$</p>
<p>前面的交叉熵损失计算都是针对单个样本的，对于<strong>所有样本</strong>，二分类的交叉熵损失计算如下：</p>
<p>$$L = \frac{1}{N}(\sum_{y_i = 1}^{m}-log(p)-\sum_{y_i = 0}^{n}log(1-p))$$</p>
<p>其中 $m$ 为正样本个数，$n$ 为负样本个数，$N$ 为样本总数，$m+n=N$。当样本类别不平衡时，损失函数 $L$ 的分布也会发生倾斜，如 $m \ll n$ 时，负样本的损失会在总损失占主导地位。又因为损失函数的倾斜，模型训练过程中也会倾向于样本多的类别，造成模型对少样本类别的性能较差。</p>
<p>再衍生以下，对于<strong>所有样本</strong>，多分类的交叉熵损失计算如下：</p>
<p>$$L = \frac{1}{N} \sum_i^N L_i = -\frac{1}{N}(\sum_i \sum_{c=1}^M y_{ic}log(p_{ic})$$</p>
<p>其中，$M$ 表示类别数量，$y_{ic}$ 是符号函数，如果样本 $i$ 的真实类别等于 $c$ 取值 1，否则取值 0; $p_{ic}$ 表示样本 $i$ 预测为类别 $c$ 的概率。</p>
<p>对于多分类问题，交叉熵损失一般会结合 <code>softmax</code> 激活一起实现，<code>PyTorch</code> 代码如下，代码出自<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/FGyV763yIKsXNM40lMO61g">这里</a>。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 交叉熵损失</span>
<span class="token keyword">class</span> <span class="token class-name">CrossEntropyLoss</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    对最后一层的神经元输出计算交叉熵损失
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>X <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>labels <span class="token operator">=</span> <span class="token boolean">None</span>
    
    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        参数：
            X: 模型最后fc层输出
            labels: one hot标注，shape=(batch_size, num_class)
        """</span>
        self<span class="token punctuation">.</span>X <span class="token operator">=</span> X
        self<span class="token punctuation">.</span>labels <span class="token operator">=</span> labels

        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        计算交叉熵损失
        参数：
            X：最后一层神经元输出，shape=(batch_size, C)
            label：数据onr-hot标注，shape=(batch_size, C)
        return：
            交叉熵loss
        """</span>
        self<span class="token punctuation">.</span>softmax_x <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        log_softmax <span class="token operator">=</span> self<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>softmax_x<span class="token punctuation">)</span>
        cross_entropy_loss <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>labels <span class="token operator">*</span> log_softmax<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> cross_entropy_loss
    
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        grad_x <span class="token operator">=</span>  <span class="token punctuation">(</span>self<span class="token punctuation">.</span>softmax_x <span class="token operator">-</span> self<span class="token punctuation">.</span>labels<span class="token punctuation">)</span>  <span class="token comment"># 返回的梯度需要除以batch_size</span>
        <span class="token keyword">return</span> grad_x <span class="token operator">/</span> self<span class="token punctuation">.</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        
    <span class="token keyword">def</span> <span class="token function">log_softmax</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> softmax_x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        参数:
            softmax_x, 在经过softmax处理过的X
        return: 
            log_softmax处理后的结果shape = (m, C)
        """</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>softmax_x <span class="token operator">+</span> <span class="token number">1e-5</span><span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        根据输入，返回softmax
        代码利用softmax函数的性质: softmax(x) = softmax(x + c)
        """</span>
        batch_size <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token comment"># axis=1 表示在二维数组中沿着横轴进行取最大值的操作</span>
        max_value <span class="token operator">=</span> X<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment">#每一行减去自己本行最大的数字,防止取指数后出现inf，性质：softmax(x) = softmax(x + c)</span>
        <span class="token comment"># 一定要新定义变量，不要用-=，否则会改变输入X。因为在调用计算损失时，多次用到了softmax，input不能改变</span>
        tmp <span class="token operator">=</span> X <span class="token operator">-</span> max_value<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 对每个数取指数</span>
        exp_input <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span>  <span class="token comment"># shape=(m, n)</span>
        <span class="token comment"># 求出每一行的和</span>
        exp_sum <span class="token operator">=</span> exp_input<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># shape=(m, 1)</span>
        <span class="token keyword">return</span> exp_input <span class="token operator">/</span> exp_sum<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="4-2，Balanced-Cross-Entropy"><a href="#4-2，Balanced-Cross-Entropy" class="headerlink" title="4.2，Balanced Cross Entropy"></a>4.2，Balanced Cross Entropy</h3><p>对于正负样本不平衡的问题，较为普遍的做法是引入 $\alpha \in(0,1)$ 参数来解决，上面公式重写如下：</p>
<p>$$CE(p_t) = -\alpha log(p_t) = \left{\begin{matrix}<br>-\alpha log(p), &amp; if \quad y=1\<br>-(1-\alpha)log(1-p), &amp;  if\quad y=0<br>\end{matrix}\right.$$</p>
<p>对于所有样本，二分类的平衡交叉熵损失函数如下：</p>
<p>$$L = \frac{1}{N}(\sum_{y_i = 1}^{m}-\alpha log(p)-\sum_{y_i = 0}^{n}(1 - \alpha) log(1-p))$$</p>
<p>其中 $\frac{\alpha}{1-\alpha} = \frac{n}{m}$，即 $\alpha$ 参数的值是根据正负样本分布比例来决定的，</p>
<h3 id="4-3，Focal-Loss-Definition"><a href="#4-3，Focal-Loss-Definition" class="headerlink" title="4.3，Focal Loss Definition"></a>4.3，Focal Loss Definition</h3><p>虽然 $\alpha$ 参数平衡了正负样本（<code>positive/negative examples</code>），但是它并不能区分难易样本（<code>easy/hard examples</code>），而实际上，目标检测中大量的候选目标都是易分样本。这些样本的损失很低，但是由于难易样本数量极不平衡，易分样本的数量相对来讲太多，最终主导了总的损失。而本文的作者认为，易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本（这个假设是有问题的，是 <code>GHM</code> 的主要改进对象）</p>
<p><code>Focal Loss</code> 作者建议在交叉熵损失函数上加上一个调整因子（<code>modulating factor</code>）$(1-p_t)^\gamma$，把高置信度 $p$（易分样本）样本的损失降低一些。<code>Focal Loss</code> 定义如下：</p>
<p>$$FL(p_t) = -(1-p_t)^\gamma log(p_t) = \left{\begin{matrix}<br>-(1-p)^\gamma log(p), &amp; if \quad y=1 \<br>-p^\gamma log(1-p), &amp;  if\quad y=0<br>\end{matrix}\right.$$</p>
<p><code>Focal Loss</code> 有两个性质：</p>
<ul>
<li>当样本被错误分类且 $p_t$ 值较小时，调制因子接近于 <code>1</code>，<code>loss</code> 几乎不受影响；当 $p_t$ 接近于 <code>1</code>，调质因子（<code>factor</code>）也接近于 <code>0</code>，<strong>容易分类样本的损失被减少了权重</strong>，整体而言，相当于增加了分类不准确样本在损失函数中的权重。</li>
<li>$\gamma$ 参数平滑地调整容易样本的权重下降率，当 $\gamma = 0$ 时，<code>Focal Loss</code> 等同于 <code>CE Loss</code>。 $\gamma$ 在增加，调制因子的作用也就增加，实验证明  $\gamma = 2$ 时，模型效果最好。</li>
</ul>
<p>直观地说，<strong>调制因子减少了简单样本的损失贡献，并扩大了样本获得低损失的范围</strong>。例如，当$\gamma = 2$ 时，与 $CE$ 相比，分类为 $p_t = 0.9$ 的样本的损耗将降低 <code>100</code> 倍，而当 $p_t = 0.968$ 时，其损耗将降低 <code>1000</code> 倍。这反过来又增加了错误分类样本的重要性（对于 $pt≤0.5$ 和 $\gamma = 2$，其损失最多减少 <code>4</code> 倍）。在训练过程关注对象的排序为正难 &gt; 负难 &gt; 正易 &gt; 负易。</p>
<p><img src="../../data/images/%E9%9A%BE%E6%98%93%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC.jpg" alt="难易正负样本"></p>
<p>在实践中，我们常采用带 $\alpha$ 的 <code>Focal Loss</code>：</p>
<p>$$FL(p_t) = -\alpha (1-p_t)^\gamma log(p_t)$$</p>
<p>作者在实验中采用这种形式，发现它比非 $\alpha$ 平衡形式（non-$\alpha$-balanced）的精确度稍有提高。实验表明 $\gamma$ 取 2，$\alpha$ 取 0.25 的时候效果最佳。</p>
<p>网上有各种版本的 <code>Focal Loss</code> 实现代码，大多都是基于某个深度学习框架实现的，如 <code>Pytorch</code>和 <code>TensorFlow</code>，我选取了一个较为清晰的通用版本代码作为参考，代码来自 <a target="_blank" rel="noopener" href="https://github.com/yatengLG/Retinanet-Pytorch/blob/master/Model/struct/Focal_Loss.py">这里</a>。</p>
<blockquote>
<p>后续有必要自己实现以下，有时间还要去看看 <code>Caffe</code> 的实现。这里的 Focal Loss 代码与后文不同，这里只是纯粹的用于分类的 Focal_loss 代码，不包含 BBox 的编码过程。</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># -*- coding: utf-8 -*-</span>
<span class="token comment"># @Author  : LG</span>
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">focal_loss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> num_classes <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span> size_average<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        focal_loss损失函数, -α(1-yi)**γ *ce_loss(xi,yi)
        步骤详细的实现了 focal_loss损失函数.
        :param alpha:   阿尔法α,类别权重.      当α是列表时,为各类别权重,当α为常数时,类别权重为[α, 1-α, 1-α, ....],常用于 目标检测算法中抑制背景类 , retainnet中设置为0.25
        :param gamma:   伽马γ,难易样本调节参数. retainnet中设置为2
        :param num_classes:     类别数量
        :param size_average:    损失计算方式,默认取均值
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>focal_loss<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>size_average <span class="token operator">=</span> size_average
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>alpha<span class="token punctuation">,</span><span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>alpha<span class="token punctuation">)</span><span class="token operator">==</span>num_classes   <span class="token comment"># α可以以list方式输入,size:[num_classes] 用于对不同类别精细地赋予权重</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" --- Focal_loss alpha = &#123;&#125;, 将对每一类权重进行精细化赋值 --- "</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>alpha<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>alpha<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> alpha<span class="token operator">&lt;</span><span class="token number">1</span>   <span class="token comment">#如果α为一个常数,则降低第一类的影响,在目标检测中为第一类</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" --- Focal_loss alpha = &#123;&#125; ,将对背景类进行衰减,请在目标检测任务中使用 --- "</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>alpha<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_classes<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>alpha<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> alpha
            self<span class="token punctuation">.</span>alpha<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>alpha<span class="token punctuation">)</span> <span class="token comment"># α 最终为 [ α, 1-α, 1-α, 1-α, 1-α, ...] size:[num_classes]</span>

        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> preds<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        focal_loss损失计算
        :param preds:   预测类别. size:[B,N,C] or [B,C]    分别对应与检测与分类任务, B 批次, N检测框数, C类别数
        :param labels:  实际类别. size:[B,N] or [B]，为 one-hot 编码格式
        :return:
        """</span>
        <span class="token comment"># assert preds.dim()==2 and labels.dim()==1</span>
        preds <span class="token operator">=</span> preds<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>preds<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> self<span class="token punctuation">.</span>alpha<span class="token punctuation">.</span>to<span class="token punctuation">(</span>preds<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        preds_logsoft <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># log_softmax</span>
        preds_softmax <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>preds_logsoft<span class="token punctuation">)</span>    <span class="token comment"># softmax</span>

        preds_softmax <span class="token operator">=</span> preds_softmax<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>labels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        preds_logsoft <span class="token operator">=</span> preds_logsoft<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>labels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> self<span class="token punctuation">.</span>alpha<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>labels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>preds_softmax<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>gamma<span class="token punctuation">)</span><span class="token punctuation">,</span> preds_logsoft<span class="token punctuation">)</span>  <span class="token comment"># torch.pow((1-preds_softmax), self.gamma) 为focal loss中 (1-pt)**γ</span>

        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>alpha<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>size_average<span class="token punctuation">:</span>
            loss <span class="token operator">=</span> loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            loss <span class="token operator">=</span> loss<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>mmdetection</code> 框架给出的 <code>focal loss</code> 代码如下（有所删减）：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># This method is only for debugging</span>
<span class="token keyword">def</span> <span class="token function">py_sigmoid_focal_loss</span><span class="token punctuation">(</span>pred<span class="token punctuation">,</span>
                          target<span class="token punctuation">,</span>
                          weight<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                          gamma<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span>
                          alpha<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span>
                          reduction<span class="token operator">=</span><span class="token string">'mean'</span><span class="token punctuation">,</span>
                          avg_factor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""PyTorch version of `Focal Loss &lt;https://arxiv.org/abs/1708.02002>`_.
    Args:
        pred (torch.Tensor): The prediction with shape (N, C), C is the
            number of classes
        target (torch.Tensor): The learning label of the prediction.
        weight (torch.Tensor, optional): Sample-wise loss weight.
        gamma (float, optional): The gamma for calculating the modulating
            factor. Defaults to 2.0.
        alpha (float, optional): A balanced form for Focal Loss.
            Defaults to 0.25.
        reduction (str, optional): The method used to reduce the loss into
            a scalar. Defaults to 'mean'.
        avg_factor (int, optional): Average factor that is used to average
            the loss. Defaults to None.
    """</span>
    pred_sigmoid <span class="token operator">=</span> pred<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
    target <span class="token operator">=</span> target<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>pred<span class="token punctuation">)</span>
    pt <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> pred_sigmoid<span class="token punctuation">)</span> <span class="token operator">*</span> target <span class="token operator">+</span> pred_sigmoid <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> target<span class="token punctuation">)</span>
    focal_weight <span class="token operator">=</span> <span class="token punctuation">(</span>alpha <span class="token operator">*</span> target <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> alpha<span class="token punctuation">)</span> <span class="token operator">*</span>
                    <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> target<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> pt<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>gamma<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>binary_cross_entropy_with_logits<span class="token punctuation">(</span>
        pred<span class="token punctuation">,</span> target<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span> <span class="token operator">*</span> focal_weigh
    <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="5，代码解读"><a href="#5，代码解读" class="headerlink" title="5，代码解读"></a>5，代码解读</h2><blockquote>
<p>代码来源<a target="_blank" rel="noopener" href="https://github.com/yhenon/pytorch-retinanet">这里</a>。</p>
</blockquote>
<h3 id="5-1，Backbone"><a href="#5-1，Backbone" class="headerlink" title="5.1，Backbone"></a>5.1，Backbone</h3><p>RetinaNet 算法采用了 ResNet50 作为 Backbone, 并且考虑到整个目标检测网络比较大，前面部分网络没有进行训练，BN 也不会进行参数更新（来自 OpenMMLab 的经验）。</p>
<p>ResNet 不仅提出了残差结构，而且还提出了骨架网络设计范式即 <code>stem + n stage+ cls head</code>，对于 ResNet 而言，其实际 forward 流程是 stem -&gt; 4 个 stage -&gt; 分类 head，stem 的输出 stride 是 4，而 4 个 stage 的输出 stride 是 4,8,16,32。</p>
<blockquote>
<p><code>stride</code> 表示模型的下采样率，假设图片输入是 <code>320x320</code>，<code>stride=10</code>，那么输出特征图大小是 <code>32x32</code> ，假设每个位置 <code>anchor</code> 是 <code>9</code> 个，那么这个输出特征图就一共有 <code>32x32x9</code> 个 <code>anchor</code>。</p>
</blockquote>
<h3 id="5-2，Neck"><a href="#5-2，Neck" class="headerlink" title="5.2，Neck"></a>5.2，Neck</h3><p>ResNet 输出 4 个不同尺度的特征图（c2,c3,c4,c5），stride 分别是（4,8,16,32），通道数为（256,512,1024,2048）。</p>
<p>Neck 使用的是 <code>FPN</code> 网络，且输入是 3 个来自 ResNet 输出的特征图（c3,c4,c5），并输出 <code>5</code> 个特征图（p3,p4,p5,p6,p7），额外输出的 2 个特征图的来源是骨架网络输出，而不是 FPN 层本身输出又作为后面层的输入，并且 <code>FPN</code> 网络输出的 <code>5</code> 个特征图通道数都是 <code>256</code>。值得注意的是，**<code>Neck</code> 模块输出特征图的大小是由 <code>Backbone</code> 决定的，即输出的 <code>stride</code> 列表由 <code>Backbone</code> 确定**。<br><code>FPN</code> 结构的代码如下。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PyramidFeatures</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> C3_size<span class="token punctuation">,</span> C4_size<span class="token punctuation">,</span> C5_size<span class="token punctuation">,</span> feature_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PyramidFeatures<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># upsample C5 to get P5 from the FPN paper</span>
        self<span class="token punctuation">.</span>P5_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>C5_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P5_upsampled <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P5_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># add P5 elementwise to C4</span>
        self<span class="token punctuation">.</span>P4_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>C4_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P4_upsampled <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P4_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># add P4 elementwise to C3</span>
        self<span class="token punctuation">.</span>P3_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>C3_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P3_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># "P6 is obtained via a 3x3 stride-2 conv on C5"</span>
        self<span class="token punctuation">.</span>P6 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>C5_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># "P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6"</span>
        self<span class="token punctuation">.</span>P7_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P7_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        C3<span class="token punctuation">,</span> C4<span class="token punctuation">,</span> C5 <span class="token operator">=</span> inputs

        P5_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P5_1<span class="token punctuation">(</span>C5<span class="token punctuation">)</span>
        P5_upsampled_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P5_upsampled<span class="token punctuation">(</span>P5_x<span class="token punctuation">)</span>
        P5_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P5_2<span class="token punctuation">(</span>P5_x<span class="token punctuation">)</span>

        P4_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P4_1<span class="token punctuation">(</span>C4<span class="token punctuation">)</span>
        P4_x <span class="token operator">=</span> P5_upsampled_x <span class="token operator">+</span> P4_x
        P4_upsampled_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P4_upsampled<span class="token punctuation">(</span>P4_x<span class="token punctuation">)</span>
        P4_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P4_2<span class="token punctuation">(</span>P4_x<span class="token punctuation">)</span>

        P3_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P3_1<span class="token punctuation">(</span>C3<span class="token punctuation">)</span>
        P3_x <span class="token operator">=</span> P3_x <span class="token operator">+</span> P4_upsampled_x
        P3_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P3_2<span class="token punctuation">(</span>P3_x<span class="token punctuation">)</span>

        P6_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P6<span class="token punctuation">(</span>C5<span class="token punctuation">)</span>

        P7_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P7_1<span class="token punctuation">(</span>P6_x<span class="token punctuation">)</span>
        P7_x <span class="token operator">=</span> self<span class="token punctuation">.</span>P7_2<span class="token punctuation">(</span>P7_x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> <span class="token punctuation">[</span>P3_x<span class="token punctuation">,</span> P4_x<span class="token punctuation">,</span> P5_x<span class="token punctuation">,</span> P6_x<span class="token punctuation">,</span> P7_x<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="5-3，Head"><a href="#5-3，Head" class="headerlink" title="5.3，Head"></a>5.3，Head</h3><p><code>RetinaNet</code> 在特征提取网络 <code>ResNet-50</code> 和特征融合网络 <code>FPN</code> 后，对获得的五张特征图 <code>[P3_x, P4_x, P5_x, P6_x, P7_x]</code>，通过具有相同权重的框回归和分类子网络，获得所有框位置和类别信息。</p>
<p>目标边界框回归和分类子网络（<code>head</code> 网络）定义如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">RegressionModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_features_in<span class="token punctuation">,</span> num_anchors<span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">,</span> feature_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>RegressionModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>num_features_in<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 最后的输出层输出通道数为 num_anchors * 4</span>
        self<span class="token punctuation">.</span>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> num_anchors <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>act1<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>act2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>act3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv4<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>act4<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token comment"># out is B x C x W x H, with C = 4*num_anchors = 4*9</span>
        out <span class="token operator">=</span> out<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> out<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">ClassificationModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_features_in<span class="token punctuation">,</span> num_anchors<span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span> prior<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> feature_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ClassificationModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>num_classes <span class="token operator">=</span> num_classes
        self<span class="token punctuation">.</span>num_anchors <span class="token operator">=</span> num_anchors

        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>num_features_in<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 最后的输出层输出通道数为 num_anchors * num_classes(coco数据集9*80)</span>
        self<span class="token punctuation">.</span>conv4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> num_anchors <span class="token operator">*</span> num_classes<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>output_act <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>act1<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>act2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>act3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv4<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>act4<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>output_act<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token comment"># out is B x C x W x H, with C = n_classes + n_anchors</span>
        out1 <span class="token operator">=</span> out<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        batch_size<span class="token punctuation">,</span> width<span class="token punctuation">,</span> height<span class="token punctuation">,</span> channels <span class="token operator">=</span> out1<span class="token punctuation">.</span>shape

        out2 <span class="token operator">=</span> out1<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> width<span class="token punctuation">,</span> height<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_anchors<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_classes<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out2<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_classes<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="5-4，先验框Anchor赋值"><a href="#5-4，先验框Anchor赋值" class="headerlink" title="5.4，先验框Anchor赋值"></a>5.4，先验框Anchor赋值</h3><p>1，生成各个特征图对应原图大小的所有 <code>Anchors</code> 坐标的代码如下。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn


<span class="token keyword">class</span> <span class="token class-name">Anchors</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pyramid_levels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> sizes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> ratios<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> scales<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Anchors<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> pyramid_levels <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>pyramid_levels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> strides <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>strides <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">**</span> x <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>pyramid_levels<span class="token punctuation">]</span>
        <span class="token keyword">if</span> sizes <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>sizes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">**</span> <span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>pyramid_levels<span class="token punctuation">]</span>
        <span class="token keyword">if</span> ratios <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>ratios <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> scales <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>scales <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">**</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">/</span> <span class="token number">3.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token number">2.0</span> <span class="token operator">/</span> <span class="token number">3.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> image<span class="token punctuation">)</span><span class="token punctuation">:</span>
        
        image_shape <span class="token operator">=</span> image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        image_shape <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>image_shape<span class="token punctuation">)</span>
        image_shapes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>image_shape <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">**</span> x <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>pyramid_levels<span class="token punctuation">]</span>

        <span class="token comment"># compute anchors over all pyramid levels</span>
        all_anchors <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> p <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>pyramid_levels<span class="token punctuation">)</span><span class="token punctuation">:</span>
            anchors         <span class="token operator">=</span> generate_anchors<span class="token punctuation">(</span>base_size<span class="token operator">=</span>self<span class="token punctuation">.</span>sizes<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> ratios<span class="token operator">=</span>self<span class="token punctuation">.</span>ratios<span class="token punctuation">,</span> scales<span class="token operator">=</span>self<span class="token punctuation">.</span>scales<span class="token punctuation">)</span>
            shifted_anchors <span class="token operator">=</span> shift<span class="token punctuation">(</span>image_shapes<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>strides<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> anchors<span class="token punctuation">)</span>
            all_anchors     <span class="token operator">=</span> np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>all_anchors<span class="token punctuation">,</span> shifted_anchors<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        all_anchors <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>all_anchors<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>all_anchors<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>all_anchors<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">generate_anchors</span><span class="token punctuation">(</span>base_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> ratios<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> scales<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""生成的 `9` 个 `base anchors` 
    Generate anchor (reference) windows by enumerating aspect ratios X
    scales w.r.t. a reference window.
    """</span>

    <span class="token keyword">if</span> ratios <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        ratios <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> scales <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        scales <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">**</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">/</span> <span class="token number">3.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token number">2.0</span> <span class="token operator">/</span> <span class="token number">3.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    num_anchors <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>ratios<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>scales<span class="token punctuation">)</span>

    <span class="token comment"># initialize output anchors</span>
    anchors <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>num_anchors<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># scale base_size</span>
    anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> base_size <span class="token operator">*</span> np<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>scales<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>ratios<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T

    <span class="token comment"># compute areas of anchors</span>
    areas <span class="token operator">=</span> anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>

    <span class="token comment"># correct for ratios</span>
    anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>areas <span class="token operator">/</span> np<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>ratios<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>scales<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>ratios<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>scales<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)</span>
    anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">-=</span> np<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T
    anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">-=</span> np<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T

    <span class="token keyword">return</span> anchors

<span class="token keyword">def</span> <span class="token function">shift</span><span class="token punctuation">(</span>shape<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> anchors<span class="token punctuation">)</span><span class="token punctuation">:</span>
    shift_x <span class="token operator">=</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">*</span> stride
    shift_y <span class="token operator">=</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">*</span> stride

    shift_x<span class="token punctuation">,</span> shift_y <span class="token operator">=</span> np<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>shift_x<span class="token punctuation">,</span> shift_y<span class="token punctuation">)</span>

    shifts <span class="token operator">=</span> np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span><span class="token punctuation">(</span>
        shift_x<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> shift_y<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        shift_x<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> shift_y<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># add A anchors (1, A, 4) to</span>
    <span class="token comment"># cell K shifts (K, 1, 4) to get</span>
    <span class="token comment"># shift anchors (K, A, 4)</span>
    <span class="token comment"># reshape to (K*A, 4) shifted anchors</span>
    A <span class="token operator">=</span> anchors<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    K <span class="token operator">=</span> shifts<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    all_anchors <span class="token operator">=</span> <span class="token punctuation">(</span>anchors<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> A<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> shifts<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> K<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    all_anchors <span class="token operator">=</span> all_anchors<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>K <span class="token operator">*</span> A<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> all_anchors<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>shift</code> 函数是将 <code>generate_anchors</code> 函数生成的 <code>9</code> 个 <code>base anchors</code> 按固定长度进行平移，然后和其对应特征图的 <code>cell</code>进行对应。经过对每个特征图（<code>5</code> 个）都做类似的变换，就能生成全部<code>anchor</code>。具体过程如下图所示。</p>
<blockquote>
<p>anchor 平移图来源<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143877125">这里</a></p>
</blockquote>
<p><img src="../../data/images/retinanet/anchor%E7%9A%84%E5%B9%B3%E7%A7%BB%E5%92%8C%E5%AF%B9%E5%BA%94.jpg" alt="anchor的平移和对应"></p>
<p>2，计算得到输出特征图上面每个点对应的原图 <code>anchor </code>坐标输出特征图上面每个点对应的原图 <code>anchor </code>坐标后，就可以和 <code>gt</code> 信息计算每个 <code>anchor</code> 的正负样本属性。具体过程总结如下：</p>
<ul>
<li>如果 anchor 和所有 gt bbox 的最大 iou 值小于 0.4，那么该 anchor 就是背景样本；</li>
<li>如果 anchor 和所有 gt bbox 的最大 iou 值大于等于 0.5，那么该 anchor 就是高质量正样本；</li>
<li>如果 gt bbox 和所有 anchor 的最大 iou 值大于等于 0(可以看出每个 gt bbox 都一定有至少一个 anchor 匹配)，那么该 gt bbox 所对应的 anchor 也是正样本；</li>
<li>其余样本全部为忽略样本即 anchor 和所有 gt bbox 的最大 iou 值处于 [0.4,0.5) 区间的 anchor 为忽略样本，不计算 loss</li>
</ul>
<h3 id="5-5，BBox-Encoder-Decoder"><a href="#5-5，BBox-Encoder-Decoder" class="headerlink" title="5.5，BBox Encoder Decoder"></a>5.5，BBox Encoder Decoder</h3><p>在 <code>anchor-based</code> 算法中，为了利用 <code>anchor</code> 信息进行更快更好的收敛，一般会对 <code>head</code> 输出的 <code>bbox</code> 分支 <code>4</code> 个值进行编解码操作，作用有两个：</p>
<ul>
<li>更好的平衡分类和回归分支 <code>loss</code>，以及平衡 <code>bbox</code> 四个预测值的 <code>loss</code>。</li>
<li>训练过程中引入 <code>anchor</code> 信息，加快收敛。</li>
<li><code>RetinaNet</code> 采用的编解码函数是主流的 <code>DeltaXYWHBBoxCoder</code>，在 <code>OpenMMlab</code> 代码中的配置如下：  <pre class="line-numbers language-python" data-language="python"><code class="language-python">bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>
    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>
    target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
<p>target_means 和 target_stds 相当于对 bbox 回归的 4 个 tx ty tw th 进行变换。在不考虑 target_means 和 target_stds 情况下，其编码公式如下：</p>
<p>$$t_{x}^{\ast } = (x^{\ast }-x_{a})/w_{a}, t_{y}^{\ast}=(y^{\ast}-y_{a})/h_{a} \\<br>t_{w}^{\ast } = log(w^{\ast }/w_{a}), t_{h}^{\ast }=log(h^{\ast }/h_{a}) $$</p>
<p>${x}^{\ast },y^{\ast}$ 是 <code>gt bbox</code> 的中心 xy 坐标， $w^{\ast },h^{\ast }$ 是 gt bbox 的 wh 值， $x_{a},y_{a}$ 是 anchor 的中心 xy 坐标， $w_{a},h_{a}$ 是 anchor 的 wh 值， $t^{\ast }$ 是预测头的 <code>bbox</code> 分支输出的 <code>4</code> 个值对应的 <code>targets</code>。可以看出 $t_x,t_y$ 预测值表示 gt bbox 中心相对于 anchor 中心点的偏移，并且通过除以 anchor 的 $wh$ 进行归一化；而 $t_w,t_h$ 预测值表示 gt bbox 的 $wh$ 除以 anchor 的 $wh$，然后取 log 非线性变换即可。</p>
<blockquote>
<p>Variables $x$, $x_a$, and $x^{\ast }$ are for the predicted box, anchor box, and groundtruth box respectively (likewise for y; w; h).</p>
</blockquote>
<p>1，考虑<strong>编码</strong>过程存在 <code>target_means</code> 和 <code>target_stds</code> 情况下，则 <code>anchor</code> 的 <code>bbox</code> 对应的 <code>target</code> 编码的核心代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">dx <span class="token operator">=</span> <span class="token punctuation">(</span>gx <span class="token operator">-</span> px<span class="token punctuation">)</span> <span class="token operator">/</span> pw
dy <span class="token operator">=</span> <span class="token punctuation">(</span>gy <span class="token operator">-</span> py<span class="token punctuation">)</span> <span class="token operator">/</span> ph
dw <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>gw <span class="token operator">/</span> pw<span class="token punctuation">)</span>
dh <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>gh <span class="token operator">/</span> ph<span class="token punctuation">)</span>
deltas <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>dx<span class="token punctuation">,</span> dy<span class="token punctuation">,</span> dw<span class="token punctuation">,</span> dh<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 最后减掉均值，处于标准差</span>
means <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>means<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
stds <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>stds<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
deltas <span class="token operator">=</span> deltas<span class="token punctuation">.</span>sub_<span class="token punctuation">(</span>means<span class="token punctuation">)</span><span class="token punctuation">.</span>div_<span class="token punctuation">(</span>stds<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>2，<strong>解码</strong>过程是编码过程的反向，比较容易理解，其核心代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 先乘上 std，加上 mean</span>
means <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>means<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> deltas<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span>
stds <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>stds<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> deltas<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span>
denorm_deltas <span class="token operator">=</span> deltas <span class="token operator">*</span> stds <span class="token operator">+</span> means
dx <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>
dy <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>
dw <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>
dh <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>
<span class="token comment"># wh 解码</span>
gw <span class="token operator">=</span> pw <span class="token operator">*</span> dw<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span>
gh <span class="token operator">=</span> ph <span class="token operator">*</span> dh<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 中心点 xy 解码</span>
gx <span class="token operator">=</span> px <span class="token operator">+</span> pw <span class="token operator">*</span> dx
gy <span class="token operator">=</span> py <span class="token operator">+</span> ph <span class="token operator">*</span> dy
<span class="token comment"># 得到 x1y1x2y2 的 gt bbox 预测坐标</span>
x1 <span class="token operator">=</span> gx <span class="token operator">-</span> gw <span class="token operator">*</span> <span class="token number">0.5</span>
y1 <span class="token operator">=</span> gy <span class="token operator">-</span> gh <span class="token operator">*</span> <span class="token number">0.5</span>
x2 <span class="token operator">=</span> gx <span class="token operator">+</span> gw <span class="token operator">*</span> <span class="token number">0.5</span>
y2 <span class="token operator">=</span> gy <span class="token operator">+</span> gh <span class="token operator">*</span> <span class="token number">0.5</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="5-6，Focal-Loss"><a href="#5-6，Focal-Loss" class="headerlink" title="5.6，Focal Loss"></a>5.6，Focal Loss</h3><p>Focal Loss 属于 CE Loss 的动态加权版本，其可以根据样本的难易程度(预测值和 label 的差距可以反映)对每个样本单独加权，易学习样本在总的 <code>loss</code> 中的权重比较低，难样本权重比较高。特征图上输出的 <code>anchor</code> 坐标列表的大部分都是属于背景且易学习的样本，虽然单个 <code>loss</code> 比较小，但是由于数目众多最终会主导梯度，从而得到次优模型，而 Focal Loss 通过<strong>指数效应</strong>把大量易学习样本的权重大大降低，从而避免上述问题。</p>
<p><img src="../../data/images/retinanet/focal-loss.png" alt="focal-loss"></p>
<p>为了便于理解，先给出 <code>Focal Loss</code> 的核心代码。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pred_sigmoid <span class="token operator">=</span> pred<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># one-hot 格式</span>
target <span class="token operator">=</span> target<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>pred<span class="token punctuation">)</span>
pt <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> pred_sigmoid<span class="token punctuation">)</span> <span class="token operator">*</span> target <span class="token operator">+</span> pred_sigmoid <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> target<span class="token punctuation">)</span>
focal_weight <span class="token operator">=</span> <span class="token punctuation">(</span>alpha <span class="token operator">*</span> target <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> alpha<span class="token punctuation">)</span> <span class="token operator">*</span>
            <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> target<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> pt<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>gamma<span class="token punctuation">)</span>
loss <span class="token operator">=</span> F<span class="token punctuation">.</span>binary_cross_entropy_with_logits<span class="token punctuation">(</span>
        pred<span class="token punctuation">,</span> target<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span> <span class="token operator">*</span> focal_weight
loss <span class="token operator">=</span> weight_reduce_loss<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> reduction<span class="token punctuation">,</span> avg_factor<span class="token punctuation">)</span>
<span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>结合 <code>BBox Assigner</code>（<code>BBox</code> 正负样本确定） 和 <code>BBox Encoder</code> （<code>BBox target</code> 计算）的代码，可得完整的 <code>Focla Loss</code> 代码如下所示。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FocalLoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">#def __init__(self):</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> classifications<span class="token punctuation">,</span> regressions<span class="token punctuation">,</span> anchors<span class="token punctuation">,</span> annotations<span class="token punctuation">)</span><span class="token punctuation">:</span>
        alpha <span class="token operator">=</span> <span class="token number">0.25</span>
        gamma <span class="token operator">=</span> <span class="token number">2.0</span>
        batch_size <span class="token operator">=</span> classifications<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        classification_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        regression_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        anchor <span class="token operator">=</span> anchors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

        anchor_widths  <span class="token operator">=</span> anchor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">-</span> anchor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        anchor_heights <span class="token operator">=</span> anchor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> anchor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
        anchor_ctr_x   <span class="token operator">=</span> anchor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> anchor_widths
        anchor_ctr_y   <span class="token operator">=</span> anchor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> anchor_heights

        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>

            classification <span class="token operator">=</span> classifications<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
            regression <span class="token operator">=</span> regressions<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

            bbox_annotation <span class="token operator">=</span> annotations<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
            bbox_annotation <span class="token operator">=</span> bbox_annotation<span class="token punctuation">[</span>bbox_annotation<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

            classification <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>classification<span class="token punctuation">,</span> <span class="token number">1e-4</span><span class="token punctuation">,</span> <span class="token number">1.0</span> <span class="token operator">-</span> <span class="token number">1e-4</span><span class="token punctuation">)</span>

            <span class="token keyword">if</span> bbox_annotation<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    alpha_factor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>classification<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> alpha

                    alpha_factor <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">-</span> alpha_factor
                    focal_weight <span class="token operator">=</span> classification
                    focal_weight <span class="token operator">=</span> alpha_factor <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>focal_weight<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span>

                    bce <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> classification<span class="token punctuation">)</span><span class="token punctuation">)</span>

                    <span class="token comment"># cls_loss = focal_weight * torch.pow(bce, gamma)</span>
                    cls_loss <span class="token operator">=</span> focal_weight <span class="token operator">*</span> bce
                    classification_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cls_loss<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                    regression_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    alpha_factor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>classification<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">*</span> alpha

                    alpha_factor <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">-</span> alpha_factor
                    focal_weight <span class="token operator">=</span> classification
                    focal_weight <span class="token operator">=</span> alpha_factor <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>focal_weight<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span>

                    bce <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> classification<span class="token punctuation">)</span><span class="token punctuation">)</span>

                    <span class="token comment"># cls_loss = focal_weight * torch.pow(bce, gamma)</span>
                    cls_loss <span class="token operator">=</span> focal_weight <span class="token operator">*</span> bce
                    classification_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cls_loss<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                    regression_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

                <span class="token keyword">continue</span>

            IoU <span class="token operator">=</span> calc_iou<span class="token punctuation">(</span>anchors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bbox_annotation<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># num_anchors x num_annotations</span>

            IoU_max<span class="token punctuation">,</span> IoU_argmax <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>IoU<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># num_anchors x 1</span>

            <span class="token comment">#import pdb</span>
            <span class="token comment">#pdb.set_trace()</span>

            <span class="token comment"># compute the loss for classification</span>
            targets <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>classification<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token operator">-</span><span class="token number">1</span>

            <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                targets <span class="token operator">=</span> targets<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

            targets<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>lt<span class="token punctuation">(</span>IoU_max<span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>

            positive_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>ge<span class="token punctuation">(</span>IoU_max<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span>

            num_positive_anchors <span class="token operator">=</span> positive_indices<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

            assigned_annotations <span class="token operator">=</span> bbox_annotation<span class="token punctuation">[</span>IoU_argmax<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

            targets<span class="token punctuation">[</span>positive_indices<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
            targets<span class="token punctuation">[</span>positive_indices<span class="token punctuation">,</span> assigned_annotations<span class="token punctuation">[</span>positive_indices<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>

            <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                alpha_factor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>targets<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> alpha
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                alpha_factor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>targets<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">*</span> alpha

            alpha_factor <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span> alpha_factor<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">-</span> alpha_factor<span class="token punctuation">)</span>
            focal_weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">-</span> classification<span class="token punctuation">,</span> classification<span class="token punctuation">)</span>
            focal_weight <span class="token operator">=</span> alpha_factor <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>focal_weight<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span>

            bce <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span>targets <span class="token operator">*</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>classification<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> targets<span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> classification<span class="token punctuation">)</span><span class="token punctuation">)</span>

            <span class="token comment"># cls_loss = focal_weight * torch.pow(bce, gamma)</span>
            cls_loss <span class="token operator">=</span> focal_weight <span class="token operator">*</span> bce

            <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                cls_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ne<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cls_loss<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>cls_loss<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                cls_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ne<span class="token punctuation">(</span>targets<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cls_loss<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>cls_loss<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>

            classification_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cls_loss<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span>torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>num_positive_anchors<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

            <span class="token comment"># compute the loss for regression</span>

            <span class="token keyword">if</span> positive_indices<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
                assigned_annotations <span class="token operator">=</span> assigned_annotations<span class="token punctuation">[</span>positive_indices<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

                anchor_widths_pi <span class="token operator">=</span> anchor_widths<span class="token punctuation">[</span>positive_indices<span class="token punctuation">]</span>
                anchor_heights_pi <span class="token operator">=</span> anchor_heights<span class="token punctuation">[</span>positive_indices<span class="token punctuation">]</span>
                anchor_ctr_x_pi <span class="token operator">=</span> anchor_ctr_x<span class="token punctuation">[</span>positive_indices<span class="token punctuation">]</span>
                anchor_ctr_y_pi <span class="token operator">=</span> anchor_ctr_y<span class="token punctuation">[</span>positive_indices<span class="token punctuation">]</span>

                gt_widths  <span class="token operator">=</span> assigned_annotations<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">-</span> assigned_annotations<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
                gt_heights <span class="token operator">=</span> assigned_annotations<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> assigned_annotations<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
                gt_ctr_x   <span class="token operator">=</span> assigned_annotations<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> gt_widths
                gt_ctr_y   <span class="token operator">=</span> assigned_annotations<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> gt_heights

                <span class="token comment"># clip widths to 1</span>
                gt_widths  <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>gt_widths<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
                gt_heights <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>gt_heights<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

                targets_dx <span class="token operator">=</span> <span class="token punctuation">(</span>gt_ctr_x <span class="token operator">-</span> anchor_ctr_x_pi<span class="token punctuation">)</span> <span class="token operator">/</span> anchor_widths_pi
                targets_dy <span class="token operator">=</span> <span class="token punctuation">(</span>gt_ctr_y <span class="token operator">-</span> anchor_ctr_y_pi<span class="token punctuation">)</span> <span class="token operator">/</span> anchor_heights_pi
                targets_dw <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>gt_widths <span class="token operator">/</span> anchor_widths_pi<span class="token punctuation">)</span>
                targets_dh <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>gt_heights <span class="token operator">/</span> anchor_heights_pi<span class="token punctuation">)</span>

                targets <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>targets_dx<span class="token punctuation">,</span> targets_dy<span class="token punctuation">,</span> targets_dw<span class="token punctuation">,</span> targets_dh<span class="token punctuation">)</span><span class="token punctuation">)</span>
                targets <span class="token operator">=</span> targets<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span>

                <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    targets <span class="token operator">=</span> targets<span class="token operator">/</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    targets <span class="token operator">=</span> targets<span class="token operator">/</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

                negative_indices <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token operator">~</span>positive_indices<span class="token punctuation">)</span>

                regression_diff <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>targets <span class="token operator">-</span> regression<span class="token punctuation">[</span>positive_indices<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

                regression_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>
                    torch<span class="token punctuation">.</span>le<span class="token punctuation">(</span>regression_diff<span class="token punctuation">,</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token number">9.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token number">9.0</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>regression_diff<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    regression_diff <span class="token operator">-</span> <span class="token number">0.5</span> <span class="token operator">/</span> <span class="token number">9.0</span>
                <span class="token punctuation">)</span>
                regression_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>regression_loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    regression_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    regression_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>classification_losses<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>regression_losses<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/yhenon/pytorch-retinanet">https://github.com/yhenon/pytorch-retinanet</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143877125">RetinaNet 论文和代码详解</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/346198300">轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://dezhoutorizhao.github.io">h3110w0r1d</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://dezhoutorizhao.github.io/6-RetinaNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/">http://dezhoutorizhao.github.io/6-RetinaNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://dezhoutorizhao.github.io" target="_blank">h3110w0r1d's Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="https://strongwillpro.oss-cn-beijing.aliyuncs.com/img/QQ图片20240111033823.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-left" href="/7-YOLOv1-v5%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" title="YOLOv1-v5论文解读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">YOLOv1-v5论文解读</div></div></a><a class="next-post pull-right" href="/5-Cascade-RCNN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" title="Cascade-RCNN论文解读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Cascade-RCNN论文解读</div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="https://strongwillpro.oss-cn-beijing.aliyuncs.com/img/QQ图片20240111033823.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">h3110w0r1d</div><div class="author-info-description">曲师大DebuGGer战队成员，喜欢科研，希望未来可以做一个科研家</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">113</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">64</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/dezhoutorizhao?tab=repositories"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">h3110w0r1d师傅又发布了新博客了，快去看看他学了什么新技术吧</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1%EF%BC%8C%E5%BC%95%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">1，引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%EF%BC%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">2，相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%EF%BC%8C%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">4.</span> <span class="toc-text">3，网络架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%EF%BC%8CBackbone"><span class="toc-number">5.</span> <span class="toc-text">3.1，Backbone</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2%EF%BC%8CNeck"><span class="toc-number">5.1.</span> <span class="toc-text">3.2，Neck</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3%EF%BC%8CHead"><span class="toc-number">5.2.</span> <span class="toc-text">3.3，Head</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%EF%BC%8CFocal-Loss"><span class="toc-number">6.</span> <span class="toc-text">4，Focal Loss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%EF%BC%8CCross-Entropy"><span class="toc-number">6.1.</span> <span class="toc-text">4.1，Cross Entropy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%EF%BC%8CBalanced-Cross-Entropy"><span class="toc-number">6.2.</span> <span class="toc-text">4.2，Balanced Cross Entropy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%EF%BC%8CFocal-Loss-Definition"><span class="toc-number">6.3.</span> <span class="toc-text">4.3，Focal Loss Definition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%EF%BC%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB"><span class="toc-number">7.</span> <span class="toc-text">5，代码解读</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1%EF%BC%8CBackbone"><span class="toc-number">7.1.</span> <span class="toc-text">5.1，Backbone</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2%EF%BC%8CNeck"><span class="toc-number">7.2.</span> <span class="toc-text">5.2，Neck</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3%EF%BC%8CHead"><span class="toc-number">7.3.</span> <span class="toc-text">5.3，Head</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4%EF%BC%8C%E5%85%88%E9%AA%8C%E6%A1%86Anchor%E8%B5%8B%E5%80%BC"><span class="toc-number">7.4.</span> <span class="toc-text">5.4，先验框Anchor赋值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5%EF%BC%8CBBox-Encoder-Decoder"><span class="toc-number">7.5.</span> <span class="toc-text">5.5，BBox Encoder Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-6%EF%BC%8CFocal-Loss"><span class="toc-number">7.6.</span> <span class="toc-text">5.6，Focal Loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">8.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/7-YOLOv1-v5%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" title="YOLOv1-v5论文解读">YOLOv1-v5论文解读</a><time datetime="2025-02-03T20:03:12.843Z" title="发表于 2025-02-04 04:03:12">2025-02-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/6-RetinaNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" title="RetinaNet网络详解">RetinaNet网络详解</a><time datetime="2025-02-03T20:02:51.459Z" title="发表于 2025-02-04 04:02:51">2025-02-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/5-Cascade-RCNN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" title="Cascade-RCNN论文解读">Cascade-RCNN论文解读</a><time datetime="2025-02-03T20:02:44.344Z" title="发表于 2025-02-04 04:02:44">2025-02-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/4-Mask-RCNN%E8%AF%A6%E8%A7%A3/" title="Mask-RCNN详解">Mask-RCNN详解</a><time datetime="2025-02-03T20:02:40.787Z" title="发表于 2025-02-04 04:02:40">2025-02-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/3-FPN%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" title="FPN网络详解">FPN网络详解</a><time datetime="2025-02-03T20:02:36.660Z" title="发表于 2025-02-04 04:02:36">2025-02-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2025 By h3110w0r1d</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">成功的秘诀在于兴趣</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.0.0"></script><script src="/js/main.js?v=5.0.0"></script><div class="js-pjax"></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="true" async="async"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      pjax.loadUrl('/404.html')
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>